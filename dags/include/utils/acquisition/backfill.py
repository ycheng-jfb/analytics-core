import pendulum

from edm.acquisition.configs import get_table_config
from include.config import stages
from include.utils.string import unindent_auto
from plugins.datetime_macros import utcnow_nodash


class Backfill:
    """
    Occasionally there is a problem with the replica server and we need to re-pull a range of data
    and merge it into the lake table.

    To avoid cluttering up s3 and the lake_archive table, we want to minimize the rows that we put
    into the production s3 inbound path.  So what we do is dump the interval into a temp s3 prefix,
    then load it into a temp table, then compare with the actual lake table to dump into the inbound
    s3 prefix only those rows that are not present in the lake table.

    Args:
        table_config: acquisition helper table config
        lower_bound: low watermark override
        upper_bound: high watermark override
        column: watermark filter column to use for the backfill
        timestamp: if trying to reprocess files that already were dumped to s3, you can provide the
            timestamp here.  otherwise, one is autogenerated at class init

    Examples:
        >>> backfill = Backfill(
        >>>     table_name='lake.ultra_merchant.membership',
        >>>     lower_bound="2020-04-07",
        >>>     upper_bound="2020-04-08",
        >>>     column='datetime_added',
        >>> )
        >>> backfill.to_s3(dry_run=True)
        >>> backfill.merge_to_lake(dry_run=True)

    """

    temp_table_name = "_tmp"

    def __init__(self, table_name, lower_bound, upper_bound, column, timestamp=None):
        self.table_config = table_config = get_table_config(table_name=table_name)
        self.full_target_table_name = table_config.full_target_table_name
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        ub_minus_one = pendulum.parse(upper_bound).add(days=-1).to_date_string()
        self.column = column
        self.load_job = self.table_config.to_snowflake_operator.get_job(None)
        self.date_range_str = (
            f"{lower_bound.replace('-', '')}_{ub_minus_one.replace('-', '')}"
        )
        filename = f"{self.date_range_str}.csv.gz"
        prod_files_path = self.load_job.files_path.rstrip("/")
        self.timestamp = timestamp or utcnow_nodash(0)
        backfill_prefix = (
            f"lake/tmp-backfill/{self.full_target_table_name}/{self.timestamp}"
        )
        self.backfill_key = f"{backfill_prefix}/{filename}"
        self.backfill_files_path = f"{stages.tsos_da_int_inbound}/{self.backfill_key}"
        self.merge_files_path = f"{prod_files_path}/backfill_{self.timestamp}/"

    def to_s3(self, dry_run=False):
        """
        Will dump data to ``backfill_prefix``, given lower bound, upper bound, and column.

        Args:
            dry_run: if True, will print out sql command but not execute anything

        """
        op = self.table_config.to_s3_operator
        op.key = self.backfill_key
        op.backfill(
            lower_bound=self.lower_bound,
            upper_bound=self.upper_bound,
            column_override=self.column,
            dry_run=dry_run,
        )

    def merge_to_lake(self, dry_run=False):
        """
        Load data from ``backfill_prefix`` to temp table and merge the relevant roles into s3
        inbound path, to be ingested on next scheduled lake delta load.

        Args:
            dry_run: if True, will print out sql commands but not execute anything

        """
        create_temp_table = (
            f"CREATE TEMP TABLE {self.temp_table_name} LIKE {self.load_job.stg_table};"
        )
        copy_to_temp_table = self.load_job.copy_config.dml_copy_into_table(
            table=self.temp_table_name,
            files_path=self.backfill_files_path,
            column_names=[x.name for x in self.table_config.column_list],
        )
        where_clause = "\n    AND ".join(
            [f"equal_null(t.{x}, s.{x})" for x in self.load_job.pk_col_names]
        )
        delta_column = self.table_config.column_list.delta_column_list[0].name
        if delta_column:
            where_clause += f"\n    AND t.{delta_column} >= s.{delta_column}"
        copyout_cmd = f"""
            COPY INTO '{self.merge_files_path}'
                FROM (
                    SELECT {self.load_job.insert_names_str}
                    FROM {self.temp_table_name} s
                    WHERE NOT exists(
                            SELECT
                                1
                            FROM {self.full_target_table_name} t
                            WHERE {where_clause}
                        )
                    )
                FILE_FORMAT = lake_stg.public.tsv
                HEADER = FALSE
                MAX_FILE_SIZE=500000000;
        """
        copyout_cmd = unindent_auto(copyout_cmd)
        sql = "\n".join([create_temp_table, copy_to_temp_table, copyout_cmd])
        if dry_run:
            print(sql)
        else:
            hook = self.table_config.to_snowflake_operator.snowflake_hook
            hook.execute_multiple(sql)
